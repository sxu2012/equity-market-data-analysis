# Equity Market Data Analysis
Data are collected daily on trades and quotes form multiple exchanges. This project creates data platforms and pipelines that provide insights through merging data points and calculating key indicators. It is implemented on a local spark system first on Windows 11, then deployed to Azure Databricks.
Files and folders:
1. guided-cap.drawio/guided-cap-draw.pdf: Architecture diagram
2. Azure: Folder contains all exported Jupyter notebooks on Azure Databricks:
    - screenshots.pdf: collections of screenshots on Azure Databricks and Storages to show the development process
    - ingestion.ipynb/ingestion.html: Step Two, Data ingestions; Read data from Azure Blob storage, parse CSV and JSON files, filter, then persist the spark dataframe to parquet files with partitions
    - load.ipynb/load.html: Step Three, End-of-Day Data Load; Create Spark dataframes using parquet files; Perform Data cleaning using Spark aggregation methods
    - analytics.ipynb/analytics.html: Step Four, Using SparkSQL and Python to build an ETL job that calculates key trade indicators, persist final analytics data back to Azure Blob storage
    - equity-market-analysis-daily.ipynb/equity-maret-analysis-daily.html: Step Five, Pipeline Orchestration; Create and schedule the daily job that runs the above steps.
3. Local: Folder contains Jupyter notebooks that runs on the local Spark system on Windows 11.
4. output_dir: Folder that contains the folders and files generated by running the Jupyter notebooks on the local Spark system on Windows 11.
